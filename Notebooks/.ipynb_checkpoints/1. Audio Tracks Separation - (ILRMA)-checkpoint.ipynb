{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a12d9208",
   "metadata": {},
   "source": [
    "# Separating A Capella Songs into their Separated Vocal Tracks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1caeea",
   "metadata": {},
   "source": [
    "One A Capella song is usually made up by 5-8 singers singing their respective parts together to form mainly the **lead_vocal, soprano, alto, bass, tenor, and vocal percussion.** <br>\n",
    "\n",
    "In this notebook, we aim to **train our own Machine Learning Model** to **separate these 6 main tracks** from one another, given an A Capella song audio input. We will be using a dataset with Japanese A Capella songs (Ja Capella)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2abd5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe40fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import math\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import random\n",
    "import IPython.display as ipd\n",
    "\n",
    "\n",
    "\n",
    "import scipy.signal as ss\n",
    "import IPython.display as ipd\n",
    "\n",
    "\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc31d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please ensure to have downloaded Ja Capella dataset and followed instructions from README.md before continuing\n",
    "\n",
    "with zipfile.ZipFile(\"Dataset/Jacapella.zip\", 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"Dataset/Jacapella\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e6b92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Dataset/Jacapella/meta.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c192836c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b4f436",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create df_mix to show all the combines audio of mixed voice parts for each song\n",
    "# append their file directory into the df_mix as well\n",
    "\n",
    "titles = []\n",
    "subset = []\n",
    "file_dir = []\n",
    "voice_part = []\n",
    "for index in df.index:\n",
    "    if df.iloc[index,0] not in titles:\n",
    "        titles.append(df.iloc[index,0])\n",
    "        subset.append(df.iloc[index,8])\n",
    "        voice_part.append(\"mixture\")\n",
    "        file_dir.append(f\"Dataset/Jacapella/{df.iloc[index,8]}/{df.iloc[index,0]}/mixture.wav\")\n",
    "        \n",
    "df_mix = pd.DataFrame([], columns=['title_in_en', 'subset','voice_part','audio_file_dir'])\n",
    "df_mix['title_in_en'] = titles\n",
    "df_mix['subset'] = subset\n",
    "df_mix['voice_part'] = voice_part\n",
    "df_mix['audio_file_dir'] = file_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cd193e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72750df7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Appending file directory of each vocal track\n",
    "audio_file_dir = []\n",
    "for index in df.index:\n",
    "    title = df.iloc[index,0]\n",
    "    subset = df.iloc[index,8]\n",
    "    voice = df.iloc[index,9]\n",
    "    audio_file_dir.append(\"Dataset/Jacapella/\" + str(subset) + \"/\" + str(title) + \"/\" + str(voice) + \".wav\")\n",
    "\n",
    "# Added new column in df_audio to show each audio track's directory\n",
    "df[\"audio_file_dir\"] = audio_file_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8b5049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating separate df for each vocal parts\n",
    "\n",
    "df_leadvocal = pd.DataFrame()\n",
    "df_soprano = pd.DataFrame()\n",
    "df_alto = pd.DataFrame()\n",
    "df_tenor = pd.DataFrame()\n",
    "df_bass = pd.DataFrame()\n",
    "df_percussion = pd.DataFrame()\n",
    "\n",
    "for index,value in df.iterrows():\n",
    "    if df.iloc[index, 9] == \"lead_vocal\":\n",
    "        df_leadvocal = df_leadvocal.append(value, ignore_index = True)\n",
    "    elif df.iloc[index, 9] == \"soprano\":\n",
    "        df_soprano = df_soprano.append(value, ignore_index = True)\n",
    "    elif df.iloc[index, 9] == \"alto\":\n",
    "        df_alto = df_alto.append(value, ignore_index = True)\n",
    "    elif df.iloc[index, 9] == \"tenor\":\n",
    "        df_tenor = df_tenor.append(value, ignore_index = True)\n",
    "    elif df.iloc[index, 9] == \"bass\":\n",
    "        df_bass = df_bass.append(value, ignore_index = True)\n",
    "    else:\n",
    "        df_percussion= df_percussion.append(value, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af1afcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_leadvocal.drop(columns = [\"title_in_ja\", \"lyric_writer\", \"copyright_of_lyric_writer\", \"composer\", \"copyright_of_composer\", \"arranger_in_en\", \"arranger_in_ja\", \"singer_id\", \"gender\", \"first_lang\"], axis=1, inplace = True)\n",
    "df_soprano.drop(columns = [\"title_in_ja\", \"lyric_writer\", \"copyright_of_lyric_writer\", \"composer\", \"copyright_of_composer\", \"arranger_in_en\", \"arranger_in_ja\", \"singer_id\", \"gender\", \"first_lang\"], axis=1, inplace = True)\n",
    "df_alto.drop(columns = [\"title_in_ja\", \"lyric_writer\", \"copyright_of_lyric_writer\", \"composer\", \"copyright_of_composer\", \"arranger_in_en\", \"arranger_in_ja\", \"singer_id\", \"gender\", \"first_lang\"], axis=1, inplace = True)\n",
    "df_tenor.drop(columns = [\"title_in_ja\", \"lyric_writer\", \"copyright_of_lyric_writer\", \"composer\", \"copyright_of_composer\", \"arranger_in_en\", \"arranger_in_ja\", \"singer_id\", \"gender\", \"first_lang\"], axis=1, inplace = True)\n",
    "df_bass.drop(columns = [\"title_in_ja\", \"lyric_writer\", \"copyright_of_lyric_writer\", \"composer\", \"copyright_of_composer\", \"arranger_in_en\", \"arranger_in_ja\", \"singer_id\", \"gender\", \"first_lang\"], axis=1, inplace = True)\n",
    "df_percussion.drop(columns = [\"title_in_ja\", \"lyric_writer\", \"copyright_of_lyric_writer\", \"composer\", \"copyright_of_composer\", \"arranger_in_en\", \"arranger_in_ja\", \"singer_id\", \"gender\", \"first_lang\"], axis=1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff6bf08",
   "metadata": {},
   "source": [
    "# Listen to an example of Jacapella :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8314d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listen to an example of Jacapella :D\n",
    "random.seed(0)\n",
    "rand = random. randint(1,35)\n",
    "\n",
    "print(f\"Mixture audio for {df_mix.iloc[rand,0]}\")\n",
    "ipd.Audio(df_mix.iloc[rand,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95927e27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Soprano audio for {df_soprano.iloc[rand,0]}\")\n",
    "ipd.Audio(df_soprano.iloc[rand,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0a1ce3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Alto audio for {df_alto.iloc[rand,0]}\")\n",
    "ipd.Audio(df_alto.iloc[rand,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b685b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Bass audio for {df_bass.iloc[rand,0]}\")\n",
    "ipd.Audio(df_bass.iloc[rand,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac8f6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Tenor audio for {df_tenor.iloc[rand,0]}\")\n",
    "ipd.Audio(df_tenor.iloc[rand,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3a33e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Percussion audio for {df_percussion.iloc[rand,0]}\")\n",
    "ipd.Audio(df_percussion.iloc[rand,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b51c6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Lead Vocal audio for {df_leadvocal.iloc[rand,0]}\")\n",
    "ipd.Audio(df_leadvocal.iloc[rand,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384da1ac",
   "metadata": {},
   "source": [
    "# Data Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fe2960",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Total of 7 genres, each has 5 songs.Each songs have 6 vocal parts.\n",
    "df_mix['subset'].value_counts().plot(kind='bar', figsize=(10,3))\n",
    "plt.xlabel(\"Genre\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.ylabel(\"Number of Samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a3ab07",
   "metadata": {},
   "source": [
    "**Amplitude Change for each vocal part**\n",
    "\n",
    "The graph displays the time on the horizontal (X) axis and the amplitude on the vertical (Y) axis but it doesn’t tell us what’s happening to frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a5c2f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add audio time series for each audio track\n",
    "\n",
    "def add_time_series(df):\n",
    "    \n",
    "    # audio time series describes the amplitude of the audio at different timesteps.\n",
    "    audio_time_series = []\n",
    "\n",
    "    # sampling rate, sr\n",
    "    sampling_rate = []\n",
    "\n",
    "    for index, data in df.iterrows():\n",
    "        y, sr = librosa.load(data['audio_file_dir'])\n",
    "        audio_time_series.append(y)\n",
    "        sampling_rate.append(sr)\n",
    "\n",
    "    df[\"audio_time_series\"] = audio_time_series\n",
    "    df['sampling_rate'] = sampling_rate\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e09b91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfs = [df_leadvocal, df_soprano, df_alto, df_tenor, df_bass, df_percussion, df_mix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c1c296",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for df in dfs:\n",
    "    add_time_series(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d31b1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualise Amplitude Change which is the change of pressure near the microphone or recording device \n",
    "# for different vocal parts of 1 song\n",
    "\n",
    "dfs = [df_leadvocal, df_soprano, df_alto, df_tenor, df_bass, df_percussion, df_mix]\n",
    "random.seed(0)\n",
    "rand = random. randint(1,35) \n",
    "\n",
    "def visualise_amp(df, index):\n",
    "    row = df.iloc[index, :]\n",
    "    pd.Series(row['audio_time_series']).plot(figsize=(10, 5), lw=1)\n",
    "    plt.title(f\"Amplitude change for {row['voice_part']} in {row['title_in_en']}\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.show()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af929ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dfs:\n",
    "    visualise_amp(df, rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc30afa",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0947af",
   "metadata": {},
   "source": [
    "**1. Zero Crossing Rate (ZCR)**\n",
    "\n",
    "The zero-crossing rate describes the rate at which a signal moves from positive to zero to negative or from negative to zero to positive. The feature is used in music retrieval systems to identify noisy signals.\n",
    "\n",
    "ZCR is a feature often used in signal processing and audio analysis. Change in ZCR could be due to transitioning from silence to sound, or change in pitch, or change in environmental sound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1e5826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat all dfs tgt \n",
    "\n",
    "df_all = pd.concat([df_leadvocal, df_soprano, df_alto, df_tenor, df_bass, df_percussion, df_mix], axis=0, ignore_index = True)\n",
    "df_all['voice_part'].nunique()\n",
    "\n",
    "df_all.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fbb693",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def zcr(df):\n",
    "    ZCR = []\n",
    "    for index, song in df.iterrows():\n",
    "        zero_crossings = librosa.zero_crossings(song['audio_time_series'],pad=False)\n",
    "        ZCR.append(sum(zero_crossings))\n",
    "\n",
    "    df[\"zcr\"] = ZCR\n",
    "    return df\n",
    "\n",
    "zcr(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a546ff37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot 2-D btw voice part & ZCR\n",
    "#df_audio[['subset', 'zcr']].plot(kind='scatter', x='subset', y='zcr')\n",
    "\n",
    "sns.scatterplot(x='subset', y='zcr', hue='voice_part', data=df_all)\n",
    "sns.set(rc = {'figure.figsize':(10, 10)})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d3bf01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add zcr into each individual dfs\n",
    "for df in dfs:\n",
    "    zcr(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29591a95",
   "metadata": {},
   "source": [
    "**2. Spectral Centroid** <br>\n",
    "\n",
    "It indicates where the ”centre of mass” for a sound is located and is calculated as the weighted mean of the frequencies present in the sound. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed98f8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#spectral centroid -- centre of mass -- weighted mean of the frequencies present in the sound\n",
    "import sklearn\n",
    "\n",
    "def spectral_centr(df):\n",
    "    num_frames = []\n",
    "    centroids = []\n",
    "    for index, data in df.iterrows():\n",
    "        x = data['audio_time_series']\n",
    "        spectral_centroids = librosa.feature.spectral_centroid(y=x, sr=data['sampling_rate'])\n",
    "#         print(spectral_centroids.shape[1])\n",
    "#         print(spectral_centroids[0])\n",
    "        num_frames.append(spectral_centroids.shape[1])\n",
    "\n",
    "        # Computing the time variable for visualization\n",
    "        frames = range(len(spectral_centroids[0]))\n",
    "        t = librosa.frames_to_time(frames)\n",
    "\n",
    "        centroids.append([spectral_centroids[0],t])\n",
    "        \n",
    "    df['num_frames'] = num_frames\n",
    "    df['spectral_centroid'] = centroids\n",
    "        \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92208878",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dfs:\n",
    "    spectral_centr(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3943c0cc",
   "metadata": {},
   "source": [
    "**3. MFCC — Mel-Frequency Cepstral Coefficients** <br>\n",
    "\n",
    "MFCCs of a signal are a small set of features (usually about 10–20) which concisely describe the overall shape of a spectral envelope. \n",
    "\n",
    "MFCC is used for deduction of noise in audios and also used for audio classification. They represent the audio's spectral characteristics and are commonly used in audio processing tasks such as music information retrieval and speech recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594b7957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mfcc(df):\n",
    "    mfcc_coefficient = []\n",
    "    for index, data in df.iterrows():\n",
    "        mfcc = librosa.feature.mfcc(y= data['audio_time_series'], sr= data['sampling_rate'])\n",
    "        mfcc_coefficient.append(mfcc)\n",
    "        \n",
    "    df['mfcc'] = mfcc_coefficient\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56dfa1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for df in dfs:\n",
    "    mfcc(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be8cfce",
   "metadata": {},
   "source": [
    "**4. Short-Term Fourier Transform (STFT)** <br>\n",
    "\n",
    "Its complex-valued coefficients provide the frequency and phase content of local sections of a signal as it evolves over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf25923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stft(df):\n",
    "    stft = []\n",
    "    for index, data in df.iterrows():\n",
    "        \n",
    "        # Return the complex Short Term Fourier Transform\n",
    "        y = data['audio_time_series']\n",
    "        sound_stft = np.abs(librosa.stft(y))\n",
    "        stft.append(sound_stft)\n",
    "                                       \n",
    "    df['stft'] = stft\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b182f4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dfs:\n",
    "    stft(df)\n",
    "    print( stft(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5e9f67",
   "metadata": {},
   "source": [
    "**Magnitude Spectrogram**\n",
    "\n",
    "A spectrogram is a detailed view of a signal that covers all three characteristics of sound.<br> \n",
    "X-axis represents time, Y-axis represent frequency, color represents amplitude. \n",
    "\n",
    "The louder the event the brighter the color, while silence is represented by black."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0fc9b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Magnitude spectrogram that visualises the 3 aspects of STFT\n",
    "\n",
    "random.seed(0)\n",
    "rand = random. randint(1,35) \n",
    "\n",
    "def spectrogram(df, index):\n",
    "    row = df.iloc[index, :]\n",
    "#     X = librosa.stft(row['audio_time_series'])\n",
    "    Xdb = librosa.amplitude_to_db(abs(row['stft']))\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    librosa.display.specshow(Xdb, sr=row['sampling_rate'], x_axis='time', y_axis='hz') \n",
    "    plt.title(f\"Spectrogram for {row['voice_part']} in {row['title_in_en']}\")\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e336fe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for df in dfs:\n",
    "    spectrogram(df, rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0149818",
   "metadata": {},
   "source": [
    "**5. Pitch Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6f7a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA on pitch features\n",
    "# Lists to store pitch analysis results\n",
    "\n",
    "def extract_pitch_and_analyze(df):\n",
    "    mean_pitch_list = []\n",
    "    median_pitch_list = []\n",
    "    min_pitch_list = []\n",
    "    max_pitch_list = []\n",
    "#     pitch_compile = []\n",
    "    \n",
    "    # Function to extract pitch and perform analysis\n",
    "    for index, data in df.iterrows():\n",
    "        # Extract pitch features using the YIN algorithm\n",
    "        pitch, magnitudes = librosa.piptrack(y=data['audio_time_series'], sr=data['sampling_rate'])\n",
    "        pitch = pitch[pitch > 0]  # Filter out non-positive pitch values\n",
    "#         pitch_compile.append(pitch)\n",
    "        \n",
    "        # Calculate pitch statistics\n",
    "        mean_pitch = pitch.mean()\n",
    "        median_pitch = np.median(pitch)\n",
    "        min_pitch = pitch.min()\n",
    "        max_pitch = pitch.max()\n",
    "\n",
    "        # Append pitch statistics to the respective lists\n",
    "        mean_pitch_list.append(mean_pitch)\n",
    "        median_pitch_list.append(median_pitch)\n",
    "        min_pitch_list.append(min_pitch)\n",
    "        max_pitch_list.append(max_pitch)\n",
    "    \n",
    "#     df['pitch'] = pitch\n",
    "    df['mean_pitch'] = mean_pitch_list\n",
    "    df['median_pitch'] = median_pitch_list\n",
    "    df['min_pitch'] = min_pitch_list\n",
    "    df['max_pitch'] = max_pitch_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17dc859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc mean pitch for individual vocal parts and put into their own dataframes.\n",
    "for df in dfs:\n",
    "    extract_pitch_and_analyze(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e16d1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine mean pitch for all vocal parts tgt to show histogram\n",
    "extract_pitch_and_analyze(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7054e02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# histogram with hue based on 'voice_part' for mean pitch\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=df_all, x='mean_pitch', hue='voice_part', element='step', bins=20, kde=True, hue_order=df_all['voice_part'].unique())\n",
    "plt.title(\"Histogram of Mean Pitch by Voice Part\")\n",
    "plt.xlabel(\"Mean Pitch\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend(title=\"Voice Part\", labels=df_all['voice_part'].unique())\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5bf947",
   "metadata": {},
   "source": [
    "**6. Chroma feature extraction** <br>\n",
    "\n",
    "Chroma feature extraction is a technique commonly used in music signal processing to represent the harmonic content of an audio signal. <br> It aims to capture the distribution of pitch classes, which are the 12 distinct notes in the Western music scale (C, C#, D, D#, E, F, F#, G, G#, A, A#, B)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febbffc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chroma(df):\n",
    "    chroma_feature = []\n",
    "    for index, data in df.iterrows():\n",
    "        # Compute chroma feature\n",
    "        chroma = librosa.feature.chroma_stft(y=data['audio_time_series'], sr=data['sampling_rate'])\n",
    "        chroma_feature.append(chroma)\n",
    "    df['chroma'] = chroma_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746046d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dfs:\n",
    "    chroma(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f7fdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "rand = random. randint(1,35) \n",
    "\n",
    "def visualise_chroma(df, index):\n",
    "    # Get the randomly selected song title row's data\n",
    "    row = df.iloc[index,:]\n",
    "    \n",
    "    # Visualize the Chroma feature matrix for diff parts of that same song\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    librosa.display.specshow(row['chroma'], y_axis='chroma', x_axis='time')\n",
    "    plt.colorbar()\n",
    "    plt.title(f\"Chroma Feature for {row['voice_part']} in {row['title_in_en']} \")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Chroma\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555fa5a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for df in dfs:\n",
    "    visualise_chroma(df, rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a2b73c",
   "metadata": {},
   "source": [
    "# Non-negative matrix factorization (NMF) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40186fa",
   "metadata": {},
   "source": [
    "NMF is a powerful sound source separation technique that can extract individual sound sources from a mixture of sounds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf76b6b",
   "metadata": {},
   "source": [
    "#### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a06ce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "###initial signal waveform\n",
    "\n",
    "df_mix.iloc[rand,0]\n",
    "\n",
    "# Plotting the sound's signal waveform\n",
    "fig, ax = plt.subplots(figsize=(10, 3))\n",
    "librosa.display.waveshow(df_mix.iloc[0,4], sr=22050, ax=ax,x_axis='time')\n",
    "ax.set(title='The sound waveform', xlabel='Time [s]')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438c134a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stft_1 = df['stft'][0]\n",
    "stft_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fc73e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_Xdb = librosa.amplitude_to_db(abs(stft_1))\n",
    "plt.figure(figsize=(10, 5))\n",
    "librosa.display.specshow(example_Xdb, sr=22050, x_axis='time', y_axis='hz') \n",
    "\n",
    "\n",
    "plt.title(f\"Spectrogram \")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded09b43",
   "metadata": {},
   "source": [
    "### NMF Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0283ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NMF Method 1: Just trying  NMF seperation\n",
    "\n",
    "#1 seperate\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "n_components = 2  # Number of components, adjust as needed\n",
    "nmf = NMF(n_components=n_components, init='nndsvd', max_iter=200)\n",
    "W = nmf.fit_transform(abs(stft_1))\n",
    "H = nmf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2bd47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the basis matrix (W)\n",
    "plt.figure(figsize=(8, 4))\n",
    "librosa.display.specshow(librosa.amplitude_to_db(W.T), x_axis='time', y_axis='log')\n",
    "plt.colorbar(format='%+5.0f dB')\n",
    "plt.title('Basis Matrix (W)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2f2957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the activation matrix (H)\n",
    "plt.figure(figsize=(8, 4))\n",
    "librosa.display.specshow(H, x_axis='time')\n",
    "plt.colorbar()\n",
    "plt.title('Activation Matrix (H)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00c614d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 reconstruct \n",
    "\n",
    "# Select the basis vector and activation coefficient for the vocals\n",
    "vocals_basis_vector = W[:, 1]  # Adjust the index as needed\n",
    "vocals_activation_coefficient = H[1, :]\n",
    "\n",
    "# Reconstruct the separated vocals\n",
    "separated_vocals = vocals_basis_vector[:, np.newaxis] @ vocals_activation_coefficient[np.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346b9b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 invert\n",
    "separated_audio = librosa.istft(separated_vocals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b095840e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 save seperate vocals\n",
    "import soundfile as sf\n",
    "sf.write('attempt_1.wav', separated_audio, 22050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432cc5e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738107ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Method 2: try optimized seperation using a cost function\n",
    "#https://medium.com/@zahrahafida.benslimane/audio-source-separation-using-non-negative-matrix-factorization-nmf-a8b204490c7d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39fe6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divergence(V,W,H, beta = 2):\n",
    "    \n",
    "    \"\"\"\n",
    "    beta = 2 : Euclidean cost function\n",
    "    beta = 1 : Kullback-Leibler cost function\n",
    "    beta = 0 : Itakura-Saito cost function\n",
    "    \"\"\" \n",
    "    \n",
    "    if beta == 0 : return np.sum( V/(W@H) - math.log10(V/(W@H)) -1 )\n",
    "    \n",
    "    if beta == 1 : return np.sum( V*math.log10(V/(W@H)) + (W@H - V))\n",
    "    \n",
    "    if beta == 2 : return 1/2*np.linalg.norm(W@H-V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307e5135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NMF(V, S, beta = 2,  threshold = 0.05, MAXITER = 5000): \n",
    "    \n",
    "    \"\"\"\n",
    "    inputs : \n",
    "    --------\n",
    "        V         : Mixture signal : |TFST|\n",
    "        S         : The number of sources to extract\n",
    "        beta      : Beta divergence considered, default=2 (Euclidean)\n",
    "        threshold : Stop criterion \n",
    "        MAXITER   : The number of maximum iterations, default=1000                                                     \n",
    "    \n",
    "    outputs :\n",
    "    ---------\n",
    "        W : dictionary matrix [KxS], W>=0\n",
    "        H : activation matrix [SxN], H>=0\n",
    "        cost_function : the optimised cost function over iterations\n",
    "       \n",
    "   Algorithm : \n",
    "   -----------\n",
    "   \n",
    "    1) Randomly initialize W and H matrices\n",
    "    2) Multiplicative update of W and H \n",
    "    3) Repeat step (2) until convergence or after MAXITER   \n",
    "    \"\"\"\n",
    "    \n",
    "    counter = 0\n",
    "    cost_function = []\n",
    "    beta_divergence = 1\n",
    "    \n",
    "    K, N = np.shape(V)\n",
    "    \n",
    "    # Initialisation of W and H matrices : The initialization is generally random\n",
    "    W = np.abs(np.random.normal(loc=0, scale = 2.5, size=(K,S)))    \n",
    "    H = np.abs(np.random.normal(loc=0, scale = 2.5, size=(S,N)))\n",
    "\n",
    "    while beta_divergence >= threshold and counter <= MAXITER:\n",
    "        \n",
    "        # Update of W and H\n",
    "        H *= (W.T@(((W@H)**(beta-2))*V))/(W.T@((W@H)**(beta-1)) + 10e-10)\n",
    "        W *= (((W@H)**(beta-2)*V)@H.T)/((W@H)**(beta-1)@H.T + 10e-10)\n",
    "        \n",
    "        # Compute cost function\n",
    "        beta_divergence =  divergence(V,W,H, beta = 2)\n",
    "        cost_function.append( beta_divergence )\n",
    "        counter += 1\n",
    "       \n",
    "    return W,H, cost_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37dfa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare magnitude spectrogram: np.abs(stft_1)\n",
    "\n",
    "magnitude_spectrogram = np.abs(stft_1)\n",
    "phase_spectrogram = np.angle(stft_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea7b179",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = magnitude_spectrogram + 1e-10\n",
    "beta = 2\n",
    "S = 3 \n",
    "\n",
    "# Applying the NMF function\n",
    "W, H, cost_function = NMF(V,S,beta = beta, threshold = 0.05, MAXITER = 5000) \n",
    "\n",
    "# Ploting the cost function\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(cost_function)\n",
    "plt.title(\"Cost Function\")\n",
    "plt.xlabel(\"Number of iteration\")\n",
    "plt.ylabel(f\"Beta Divergence for beta = {beta} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff12677",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization of audio sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73fa255",
   "metadata": {},
   "outputs": [],
   "source": [
    "#After NMF, each audio source S can be expressed as a frequency mask over time\n",
    "f, axs = plt.subplots(nrows=1, ncols=S,figsize=(20,5))\n",
    "filtered_spectrograms = []\n",
    "for i in range(S):\n",
    "    axs[i].set_title(f\"Frequency Mask of Audio Source s = {i+1}\") \n",
    "    # Filter eash source components\n",
    "    filtered_spectrogram = W[:,[i]]@H[[i],:]\n",
    "    # Compute the filtered spectrogram\n",
    "    D = librosa.amplitude_to_db(filtered_spectrogram, ref = np.max)\n",
    "    # Show the filtered spectrogram\n",
    "    librosa.display.specshow(D,y_axis = 'hz', sr=22050,hop_length=256,x_axis ='time',cmap= matplotlib.cm.jet, ax = axs[i])\n",
    "    \n",
    "    filtered_spectrograms.append(filtered_spectrogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ba1787",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reconstruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f206455b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_sounds = []\n",
    "for i in range(S):\n",
    "    reconstruct = filtered_spectrograms[i] * np.exp(1j*phase_spectrogram)\n",
    "    new_sound   = librosa.istft(reconstruct, hop_length = 256)\n",
    "    reconstructed_sounds.append(new_sound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087d4c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_sounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2c1624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracing the waveform\n",
    "colors = ['r', 'g','b']\n",
    "fig, ax = plt.subplots(nrows=3, ncols=1, sharex=True, figsize=(10, 8))\n",
    "for i in range(S):\n",
    "    librosa.display.waveshow(reconstructed_sounds[i], sr=22050, color = colors[i], ax=ax[i],label=f'Source {i}',x_axis='time')\n",
    "    ax[i].set(xlabel='Time [s]')\n",
    "    ax[i].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6134eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "sf.write('separated_vocals_exp_source0.wav', reconstructed_sounds[0], 22050)\n",
    "sf.write('separated_vocals_exp_source1.wav', reconstructed_sounds[1], 22050)\n",
    "sf.write('separated_vocals_exp_source2.wav', reconstructed_sounds[2], 22050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716e2fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bb65f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Attempt 3: Trying Blind Source Seperation  \n",
    "#https://sound-source-separation-python.readthedocs.io/en/v0.1.7/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068cc7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_path = df_mix.iloc[rand,3]\n",
    "final_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9f1cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import wavfile\n",
    "\n",
    "\n",
    "\n",
    "# Read the .wav files of singular sauces\n",
    "sample_rate_s, soprano_wave = wavfile.read(df_soprano.iloc[rand,3])\n",
    "sample_rate_a, alto_wave = wavfile.read(df_alto.iloc[rand,3])\n",
    "sample_rate_t, tenor_wave = wavfile.read(df_tenor.iloc[rand,3])\n",
    "sample_rate_b, bass_wave = wavfile.read(df_bass.iloc[rand,3])\n",
    "sample_rate_p, percussion_wave = wavfile.read(df_percussion.iloc[rand,3])\n",
    "sample_rate_lv, lead_vocal_wave = wavfile.read(df_leadvocal.iloc[rand,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7066ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Sample rate (soprano): {sample_rate_s}\")\n",
    "print(f\"Waveform (soprano): {soprano_wave}\")\n",
    "soprano_wave.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb482669",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming soprano_wave is your audio waveform\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(soprano_wave)\n",
    "plt.title('Soprano Audio Waveform')\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#indiv waveforms are working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d32d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "soprano_stereo_waveform = np.stack((soprano_wave, soprano_wave, soprano_wave), axis=0)\n",
    "alto_stereo_waveform = np.stack((alto_wave, alto_wave, alto_wave), axis=0)\n",
    "tenor_stereo_waveform = np.stack((tenor_wave, tenor_wave, tenor_wave), axis=0)\n",
    "bass_stereo_waveform = np.stack((bass_wave, bass_wave, bass_wave), axis=0)\n",
    "percussion_stereo_waveform = np.stack((percussion_wave, percussion_wave, percussion_wave), axis=0)\n",
    "lead_vocal_stereo_waveform = np.stack((lead_vocal_wave, lead_vocal_wave, lead_vocal_wave), axis=0)\n",
    "\n",
    "print(soprano_stereo_waveform)\n",
    "soprano_stereo_waveform.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3b8b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "tenor_stereo_waveform.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda3dfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "waveforms = []\n",
    "waveforms.append(soprano_wave)\n",
    "waveforms.append(alto_wave) \n",
    "waveforms.append(tenor_wave) \n",
    "waveforms.append(bass_wave) \n",
    "waveforms.append(percussion_wave) \n",
    "waveforms.append(lead_vocal_wave) #combine all separated audio together\n",
    "\n",
    "waveforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5dbcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "waveforms_stereo= []\n",
    "\n",
    "waveforms_stereo.append(soprano_stereo_waveform)\n",
    "waveforms_stereo.append(alto_stereo_waveform) \n",
    "waveforms_stereo.append(tenor_stereo_waveform) \n",
    "waveforms_stereo.append(bass_stereo_waveform) \n",
    "waveforms_stereo.append(percussion_stereo_waveform) \n",
    "waveforms_stereo.append(lead_vocal_stereo_waveform) #combine all separated audio together\n",
    "\n",
    "waveforms_stereo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd3baee",
   "metadata": {},
   "outputs": [],
   "source": [
    "waveforms = np.stack(waveforms, axis=1)\n",
    "print(waveforms[0:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cebb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "waveforms_stereo = np.stack(waveforms_stereo, axis=1)\n",
    "print(waveforms_stereo[0:6])\n",
    "waveforms_stereo[0:6].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672f6802",
   "metadata": {},
   "outputs": [],
   "source": [
    "waveforms_concat = soprano_wave + alto_wave + tenor_wave + bass_wave + percussion_wave + lead_vocal_wave\n",
    "waveforms_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5330760b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming soprano_wave is your audio waveform\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(waveforms_concat)\n",
    "plt.title('Mixed Audio Waveform')\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#indiv waveforms are stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037362ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mixture of waveforms: {}\")\n",
    "display(ipd.Audio(waveforms_stereo_concat, rate=sample_rate_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703f9d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshaping to get w_img form \n",
    "\n",
    "# Reshape the 1D array into a 3D array\n",
    "desired_shape = (2, 2, -1)  # Define the desired shape (2 planes, 2 rows, and -1 for automatic columns)  \n",
    "#how do i get desired shape without screwing up the waveform , duplicate into shape?\n",
    "\n",
    "reshaped_wave = waveforms_concat.reshape(desired_shape)\n",
    "\n",
    "print(reshaped_wave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34edadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get w_mix \n",
    "\n",
    "waveform_mix = np.sum(reshaped_wave, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284e3965",
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform_mix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed50f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform_mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cff4fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#slice wave \n",
    "\n",
    "# Calculate the new size (1/10th of the original size)\n",
    "new_size = waveform_mix.shape[1] // 10\n",
    "\n",
    "# Calculate the starting index to get the middle portion\n",
    "start_index = (waveform_mix.shape[1] - new_size) // 2\n",
    "\n",
    "# Calculate the ending index\n",
    "end_index = start_index + new_size\n",
    "\n",
    "# Slice the waveform to get the middle portion\n",
    "waveform_sample_mix = waveform_mix[:, start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9953b270",
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform_sample_mix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2dd173",
   "metadata": {},
   "outputs": [],
   "source": [
    "#part 2: using blind source seperation model\n",
    "\n",
    "from ssspy.bss.ilrma import GaussILRMA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f474b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "ilrma = GaussILRMA(n_basis=3, rng=np.random.default_rng(0))  # 3 basis matrices\n",
    "print(ilrma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44eba870",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fft, hop_length = 2048, 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea73f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "# Get the available physical memory in bytes\n",
    "available_memory = psutil.virtual_memory().available\n",
    "\n",
    "# Convert the available memory to a more human-readable format\n",
    "available_memory_gb = available_memory / (1024**3)  # Convert bytes to gigabytes\n",
    "\n",
    "print(f\"Available Memory: {available_memory_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5490486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, spectrogram_mix = ss.stft(waveform_sample_mix, window = \"hann\")\n",
    "#, window=window, nperseg=n_fft, noverlap=n_fft-hop_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c1600f",
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform_sample_mix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dbea50",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram_est = ilrma(spectrogram_mix, n_iter=500)\n",
    "\n",
    "\n",
    "#The shape is (n_channels, n_bins, n_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fd10c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, waveform_est = ss.istft(spectrogram_est,window = \"hann\")  # window=window, nperseg=n_fft, noverlap=n_fft-hop_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e33fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6e94d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, waveform in enumerate(waveform_est):\n",
    "    print(\"Estimated source: {}\".format(idx + 1))\n",
    "    display(ipd.Audio(waveform, rate=sample_rate_s))\n",
    "    print()    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a5cadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "\n",
    "\n",
    "# Load your sound waveform\n",
    "sound_waveform, sound_sample_rate = waveform_est[0],sample_rate_s\n",
    "\n",
    "# Load your ground truth waveform\n",
    "ground_truth_waveform, gt_sample_rate = librosa.load(df_mix.iloc[0,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665d652c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if they have the same length\n",
    "if ground_truth_waveform.shape[0] != sound_waveform.shape[0]:\n",
    "    # Trim the longer waveform to match the length of the shorter waveform\n",
    "    min_length = min(ground_truth_waveform.shape[0], sound_waveform.shape[0])\n",
    "    \n",
    "    reference_sources = ground_truth_waveform[:min_length]\n",
    "    estimated_sources = sound_waveform[:min_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013c9a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a time vector for the x-axis\n",
    "sound_duration = len(estimated_sources) / sound_sample_rate\n",
    "sound_time = librosa.times_like(estimated_sources, sr=sound_sample_rate)\n",
    "\n",
    "# Plot the sound waveform\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title('Sound Waveform')\n",
    "plt.plot(sound_time, estimated_sources, color='b')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "\n",
    "# Plot the ground truth waveform\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title('Ground Truth Waveform')\n",
    "gt_time = librosa.times_like(reference_sources, sr=gt_sample_rate)\n",
    "plt.plot(gt_time, reference_sources, color='r')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaea434",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_sources.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251d3382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mir_eval\n",
    "\n",
    "# Calculate SDR\n",
    "sdr, sir, sar, _ = mir_eval.separation.bss_eval_sources(reference_sources, estimated_sources)\n",
    "\n",
    "print(sdr, sir, sar,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53872eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(1, len(ilrma.loss)), ilrma.loss[1:])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5",
   "language": "python",
   "name": "python3.8.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
