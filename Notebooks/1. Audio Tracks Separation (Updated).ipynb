{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a12d9208",
   "metadata": {},
   "source": [
    "# Separating A Capella Songs into their Separated Vocal Tracks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1caeea",
   "metadata": {},
   "source": [
    "One A Capella song is usually made up by 5-8 singers singing their respective parts together to form mainly the **lead_vocal, soprano, alto, bass, tenor, and vocal percussion.** <br>\n",
    "\n",
    "In this notebook, we aim to **train our own Machine Learning Model** to **separate these 6 main tracks** from one another, given an A Capella song audio input. We will be using a dataset with Japanese A Capella songs (Ja Capella)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2abd5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe40fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import math\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import random\n",
    "import IPython.display as ipd\n",
    "\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc31d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please ensure to have downloaded Ja Capella dataset and followed instructions from README.md before continuing\n",
    "\n",
    "with zipfile.ZipFile(\"Dataset/Jacapella.zip\", 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"Dataset/Jacapella\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e6b92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Dataset/Jacapella/meta.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c192836c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b4f436",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create df_mix to show all the combines audio of mixed voice parts for each song\n",
    "# append their file directory into the df_mix as well\n",
    "\n",
    "titles = []\n",
    "subset = []\n",
    "file_dir = []\n",
    "voice_part = []\n",
    "for index in df.index:\n",
    "    if df.iloc[index,0] not in titles:\n",
    "        titles.append(df.iloc[index,0])\n",
    "        subset.append(df.iloc[index,8])\n",
    "        voice_part.append(\"mixture\")\n",
    "        file_dir.append(f\"Dataset/Jacapella/{df.iloc[index,8]}/{df.iloc[index,0]}/mixture.wav\")\n",
    "        \n",
    "df_mix = pd.DataFrame([], columns=['title_in_en', 'subset','voice_part','audio_file_dir'])\n",
    "df_mix['title_in_en'] = titles\n",
    "df_mix['subset'] = subset\n",
    "df_mix['voice_part'] = voice_part\n",
    "df_mix['audio_file_dir'] = file_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72750df7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Appending file directory of each vocal track\n",
    "audio_file_dir = []\n",
    "for index in df.index:\n",
    "    title = df.iloc[index,0]\n",
    "    subset = df.iloc[index,8]\n",
    "    voice = df.iloc[index,9]\n",
    "    audio_file_dir.append(\"Dataset/Jacapella/\" + str(subset) + \"/\" + str(title) + \"/\" + str(voice) + \".wav\")\n",
    "\n",
    "# Added new column in df_audio to show each audio track's directory\n",
    "df[\"audio_file_dir\"] = audio_file_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8b5049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating separate df for each vocal parts\n",
    "\n",
    "df_leadvocal = pd.DataFrame()\n",
    "df_soprano = pd.DataFrame()\n",
    "df_alto = pd.DataFrame()\n",
    "df_tenor = pd.DataFrame()\n",
    "df_bass = pd.DataFrame()\n",
    "df_percussion = pd.DataFrame()\n",
    "\n",
    "for index,value in df.iterrows():\n",
    "    if df.iloc[index, 9] == \"lead_vocal\":\n",
    "        df_leadvocal = df_leadvocal.append(value, ignore_index = True)\n",
    "    elif df.iloc[index, 9] == \"soprano\":\n",
    "        df_soprano = df_soprano.append(value, ignore_index = True)\n",
    "    elif df.iloc[index, 9] == \"alto\":\n",
    "        df_alto = df_alto.append(value, ignore_index = True)\n",
    "    elif df.iloc[index, 9] == \"tenor\":\n",
    "        df_tenor = df_tenor.append(value, ignore_index = True)\n",
    "    elif df.iloc[index, 9] == \"bass\":\n",
    "        df_bass = df_bass.append(value, ignore_index = True)\n",
    "    else:\n",
    "        df_percussion= df_percussion.append(value, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af1afcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_leadvocal.drop(columns = [\"title_in_ja\", \"lyric_writer\", \"copyright_of_lyric_writer\", \"composer\", \"copyright_of_composer\", \"arranger_in_en\", \"arranger_in_ja\", \"singer_id\", \"gender\", \"first_lang\"], axis=1, inplace = True)\n",
    "df_soprano.drop(columns = [\"title_in_ja\", \"lyric_writer\", \"copyright_of_lyric_writer\", \"composer\", \"copyright_of_composer\", \"arranger_in_en\", \"arranger_in_ja\", \"singer_id\", \"gender\", \"first_lang\"], axis=1, inplace = True)\n",
    "df_alto.drop(columns = [\"title_in_ja\", \"lyric_writer\", \"copyright_of_lyric_writer\", \"composer\", \"copyright_of_composer\", \"arranger_in_en\", \"arranger_in_ja\", \"singer_id\", \"gender\", \"first_lang\"], axis=1, inplace = True)\n",
    "df_tenor.drop(columns = [\"title_in_ja\", \"lyric_writer\", \"copyright_of_lyric_writer\", \"composer\", \"copyright_of_composer\", \"arranger_in_en\", \"arranger_in_ja\", \"singer_id\", \"gender\", \"first_lang\"], axis=1, inplace = True)\n",
    "df_bass.drop(columns = [\"title_in_ja\", \"lyric_writer\", \"copyright_of_lyric_writer\", \"composer\", \"copyright_of_composer\", \"arranger_in_en\", \"arranger_in_ja\", \"singer_id\", \"gender\", \"first_lang\"], axis=1, inplace = True)\n",
    "df_percussion.drop(columns = [\"title_in_ja\", \"lyric_writer\", \"copyright_of_lyric_writer\", \"composer\", \"copyright_of_composer\", \"arranger_in_en\", \"arranger_in_ja\", \"singer_id\", \"gender\", \"first_lang\"], axis=1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff6bf08",
   "metadata": {},
   "source": [
    "# Listen to an example of Jacapella :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8314d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listen to an example of Jacapella :D\n",
    "random.seed(0)\n",
    "rand = random. randint(1,35)\n",
    "\n",
    "print(f\"Mixture audio for {df_mix.iloc[rand,0]}\")\n",
    "ipd.Audio(df_mix.iloc[rand,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95927e27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Soprano audio for {df_soprano.iloc[rand,0]}\")\n",
    "ipd.Audio(df_soprano.iloc[rand,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0a1ce3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Alto audio for {df_alto.iloc[rand,0]}\")\n",
    "ipd.Audio(df_alto.iloc[rand,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b685b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Bass audio for {df_bass.iloc[rand,0]}\")\n",
    "ipd.Audio(df_bass.iloc[rand,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac8f6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Tenor audio for {df_tenor.iloc[rand,0]}\")\n",
    "ipd.Audio(df_tenor.iloc[rand,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3a33e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Percussion audio for {df_percussion.iloc[rand,0]}\")\n",
    "ipd.Audio(df_percussion.iloc[rand,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b51c6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Lead Vocal audio for {df_leadvocal.iloc[rand,0]}\")\n",
    "ipd.Audio(df_leadvocal.iloc[rand,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384da1ac",
   "metadata": {},
   "source": [
    "# Data Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fe2960",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Total of 7 genres, each has 5 songs.Each songs have 6 vocal parts.\n",
    "df_mix['subset'].value_counts().plot(kind='bar', figsize=(10,3))\n",
    "plt.xlabel(\"Genre\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.ylabel(\"Number of Samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a3ab07",
   "metadata": {},
   "source": [
    "**Amplitude Change for each vocal part**\n",
    "\n",
    "The graph displays the time on the horizontal (X) axis and the amplitude on the vertical (Y) axis but it doesn’t tell us what’s happening to frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a5c2f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add audio time series for each audio track\n",
    "\n",
    "def add_time_series(df):\n",
    "    \n",
    "    # audio time series describes the amplitude of the audio at different timesteps.\n",
    "    audio_time_series = []\n",
    "\n",
    "    # sampling rate, sr\n",
    "    sampling_rate = []\n",
    "\n",
    "    for index, data in df.iterrows():\n",
    "        y, sr = librosa.load(data['audio_file_dir'])\n",
    "        audio_time_series.append(y)\n",
    "        sampling_rate.append(sr)\n",
    "\n",
    "    df[\"audio_time_series\"] = audio_time_series\n",
    "    df['sampling_rate'] = sampling_rate\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e09b91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfs = [df_leadvocal, df_soprano, df_alto, df_tenor, df_bass, df_percussion, df_mix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c1c296",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for df in dfs:\n",
    "    add_time_series(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d31b1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualise Amplitude Change which is the change of pressure near the microphone or recording device \n",
    "# for different vocal parts of 1 song\n",
    "\n",
    "dfs = [df_leadvocal, df_soprano, df_alto, df_tenor, df_bass, df_percussion, df_mix]\n",
    "random.seed(0)\n",
    "rand = random. randint(1,35) \n",
    "\n",
    "def visualise_amp(df, index):\n",
    "    row = df.iloc[index, :]\n",
    "    pd.Series(row['audio_time_series']).plot(figsize=(10, 5), lw=1)\n",
    "    plt.title(f\"Amplitude change for {row['voice_part']} in {row['title_in_en']}\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.show()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af929ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dfs:\n",
    "    visualise_amp(df, rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5e9f67",
   "metadata": {},
   "source": [
    "**Magnitude Spectrogram**\n",
    "\n",
    "A spectrogram is a detailed view of a signal that covers all three characteristics of sound.<br> \n",
    "X-axis represents time, Y-axis represent frequency, color represents amplitude. \n",
    "\n",
    "The louder the event the brighter the color, while silence is represented by black."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0fc9b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#display Spectrogram of different vocal parts in a random song\n",
    "\n",
    "dfs = [df_leadvocal, df_soprano, df_alto, df_tenor, df_bass, df_percussion, df_mix]\n",
    "random.seed(0)\n",
    "rand = random. randint(1,35) \n",
    "\n",
    "def spectrogram(df, index):\n",
    "    row = df.iloc[index, :]\n",
    "    X = librosa.stft(row['audio_time_series'])\n",
    "    Xdb = librosa.amplitude_to_db(abs(X))\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    librosa.display.specshow(Xdb, sr=row['sampling_rate'], x_axis='time', y_axis='hz') \n",
    "    plt.title(f\"Spectrogram for {row['voice_part']} in {row['title_in_en']}\")\n",
    "    plt.colorbar()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e336fe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for df in dfs:\n",
    "    spectrogram(df, rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc30afa",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0947af",
   "metadata": {},
   "source": [
    "**1. Zero Crossing Rate (ZCR)**\n",
    "\n",
    "The zero-crossing rate describes the rate at which a signal moves from positive to zero to negative or from negative to zero to positive. The feature is used in music retrieval systems to identify noisy signals.\n",
    "\n",
    "ZCR is a feature often used in signal processing and audio analysis. Change in ZCR could be due to transitioning from silence to sound, or change in pitch, or change in environmental sound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1e5826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat all dfs tgt \n",
    "\n",
    "df_all = pd.concat([df_leadvocal, df_soprano, df_alto, df_tenor, df_bass, df_percussion, df_mix], axis=0, ignore_index = True)\n",
    "df_all['voice_part'].nunique()\n",
    "\n",
    "df_all.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fbb693",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def zcr(df):\n",
    "    ZCR = []\n",
    "    for index, song in df.iterrows():\n",
    "        zero_crossings = librosa.zero_crossings(song['audio_time_series'],pad=False)\n",
    "        ZCR.append(sum(zero_crossings))\n",
    "\n",
    "    df[\"zcr\"] = ZCR\n",
    "    return df\n",
    "\n",
    "zcr(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a546ff37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot 2-D btw voice part & ZCR\n",
    "#df_audio[['subset', 'zcr']].plot(kind='scatter', x='subset', y='zcr')\n",
    "\n",
    "sns.scatterplot(x='subset', y='zcr', hue='voice_part', data=df_all)\n",
    "sns.set(rc = {'figure.figsize':(10, 10)})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d3bf01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add zcr into each individual dfs\n",
    "for df in dfs:\n",
    "    zcr(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29591a95",
   "metadata": {},
   "source": [
    "**2. Spectral Centroid** <br>\n",
    "\n",
    "It indicates where the ”centre of mass” for a sound is located and is calculated as the weighted mean of the frequencies present in the sound. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed98f8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#spectral centroid -- centre of mass -- weighted mean of the frequencies present in the sound\n",
    "import sklearn\n",
    "\n",
    "def spectral_centr(df):\n",
    "    num_frames = []\n",
    "    centroids = []\n",
    "    for index, data in df.iterrows():\n",
    "        x = data['audio_time_series']\n",
    "        spectral_centroids = librosa.feature.spectral_centroid(y=x, sr=data['sampling_rate'])\n",
    "#         print(spectral_centroids.shape[1])\n",
    "#         print(spectral_centroids[0])\n",
    "        num_frames.append(spectral_centroids.shape[1])\n",
    "\n",
    "        # Computing the time variable for visualization\n",
    "        frames = range(len(spectral_centroids[0]))\n",
    "        t = librosa.frames_to_time(frames)\n",
    "\n",
    "        centroids.append([spectral_centroids[0],t])\n",
    "        \n",
    "    df['num_frames'] = num_frames\n",
    "    df['spectral_centroid'] = centroids\n",
    "        \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92208878",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dfs:\n",
    "    spectral_centr(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3943c0cc",
   "metadata": {},
   "source": [
    "**3. MFCC — Mel-Frequency Cepstral Coefficients** <br>\n",
    "\n",
    "MFCCs of a signal are a small set of features (usually about 10–20) which concisely describe the overall shape of a spectral envelope. \n",
    "\n",
    "MFCC is used for deduction of noise in audios and also used for audio classification. They represent the audio's spectral characteristics and are commonly used in audio processing tasks such as music information retrieval and speech recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594b7957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mfcc(df):\n",
    "    mfcc_coefficient = []\n",
    "    for index, data in df.iterrows():\n",
    "        mfcc = librosa.feature.mfcc(y= data['audio_time_series'], sr= data['sampling_rate'])\n",
    "        mfcc_coefficient.append(mfcc)\n",
    "        \n",
    "    df['mfcc'] = mfcc_coefficient\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56dfa1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for df in dfs:\n",
    "    mfcc(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be8cfce",
   "metadata": {},
   "source": [
    "**4. Short-Term Fourier Transform (STFT)** <br>\n",
    "\n",
    "Its complex-valued coefficients provide the frequency and phase content of local sections of a signal as it evolves over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf25923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stft(df):\n",
    "    stft = []\n",
    "    for index, data in df.iterrows():\n",
    "        \n",
    "        # Return the complex Short Term Fourier Transform\n",
    "        y = data['audio_time_series']\n",
    "        sound_stft = np.abs(librosa.stft(y))\n",
    "        stft.append(sound_stft)\n",
    "                                       \n",
    "    df['stft'] = stft\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b182f4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dfs:\n",
    "    stft(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae066fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_stft(df):\n",
    "    for index, data in df.iterrows():\n",
    "        # Magnitude Spectrogram\n",
    "        sound_stft_Magnitude = np.abs(data['stft'])\n",
    "\n",
    "        # Phase spectrogram\n",
    "        sound_stft_Angle = np.angle(data['stft'])\n",
    "\n",
    "    #Plot Spectogram\n",
    "    Spec = librosa.amplitude_to_db(sound_stft_Magnitude, ref = np.max)\n",
    "    librosa.display.specshow(Spec,y_axis = 'hz',sr=data['sampling_rate'], x_axis ='time', cmap= matplotlib.cm.jet)\n",
    "    plt.title(f\"Audio spectrogram for {data['voice_part']}\")\n",
    "    plt.figure(figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbd4b42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for df in dfs:\n",
    "    visualise_stft(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0149818",
   "metadata": {},
   "source": [
    "**5. Pitch Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6f7a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA on pitch features\n",
    "# Lists to store pitch analysis results\n",
    "\n",
    "def extract_pitch_and_analyze(df):\n",
    "    mean_pitch_list = []\n",
    "    median_pitch_list = []\n",
    "    min_pitch_list = []\n",
    "    max_pitch_list = []\n",
    "#     pitch_compile = []\n",
    "    \n",
    "    # Function to extract pitch and perform analysis\n",
    "    for index, data in df.iterrows():\n",
    "        # Extract pitch features using the YIN algorithm\n",
    "        pitch, magnitudes = librosa.piptrack(y=data['audio_time_series'], sr=data['sampling_rate'])\n",
    "        pitch = pitch[pitch > 0]  # Filter out non-positive pitch values\n",
    "#         pitch_compile.append(pitch)\n",
    "        \n",
    "        # Calculate pitch statistics\n",
    "        mean_pitch = pitch.mean()\n",
    "        median_pitch = np.median(pitch)\n",
    "        min_pitch = pitch.min()\n",
    "        max_pitch = pitch.max()\n",
    "\n",
    "        # Append pitch statistics to the respective lists\n",
    "        mean_pitch_list.append(mean_pitch)\n",
    "        median_pitch_list.append(median_pitch)\n",
    "        min_pitch_list.append(min_pitch)\n",
    "        max_pitch_list.append(max_pitch)\n",
    "    \n",
    "#     df['pitch'] = pitch\n",
    "    df['mean_pitch'] = mean_pitch_list\n",
    "    df['median_pitch'] = median_pitch_list\n",
    "    df['min_pitch'] = min_pitch_list\n",
    "    df['max_pitch'] = max_pitch_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17dc859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc mean pitch for individual vocal parts and put into their own dataframes.\n",
    "for df in dfs:\n",
    "    extract_pitch_and_analyze(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e16d1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine mean pitch for all vocal parts tgt to show histogram\n",
    "extract_pitch_and_analyze(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7054e02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# histogram with hue based on 'voice_part' for mean pitch\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=df_all, x='mean_pitch', hue='voice_part', element='step', bins=20, kde=True, hue_order=df_all['voice_part'].unique())\n",
    "plt.title(\"Histogram of Mean Pitch by Voice Part\")\n",
    "plt.xlabel(\"Mean Pitch\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend(title=\"Voice Part\", labels=df_all['voice_part'].unique())\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5bf947",
   "metadata": {},
   "source": [
    "**6. Chroma feature extraction** <br>\n",
    "\n",
    "Chroma feature extraction is a technique commonly used in music signal processing to represent the harmonic content of an audio signal. <br> It aims to capture the distribution of pitch classes, which are the 12 distinct notes in the Western music scale (C, C#, D, D#, E, F, F#, G, G#, A, A#, B)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febbffc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chroma(df):\n",
    "    chroma_feature = []\n",
    "    for index, data in df.iterrows():\n",
    "        # Compute chroma feature\n",
    "        chroma = librosa.feature.chroma_stft(y=data['audio_time_series'], sr=data['sampling_rate'])\n",
    "        chroma_feature.append(chroma)\n",
    "    df['chroma'] = chroma_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746046d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dfs:\n",
    "    chroma(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f7fdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "rand = random. randint(1,35) \n",
    "\n",
    "def visualise_chroma(df, index):\n",
    "    # Get the randomly selected song title row's data\n",
    "    row = df.iloc[index,:]\n",
    "    \n",
    "    # Visualize the Chroma feature matrix for diff parts of that same song\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    librosa.display.specshow(row['chroma'], y_axis='chroma', x_axis='time')\n",
    "    plt.colorbar()\n",
    "    plt.title(f\"Chroma Feature for {row['voice_part']} in {row['title_in_en']} \")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Chroma\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555fa5a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for df in dfs:\n",
    "    visualise_chroma(df, rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a2b73c",
   "metadata": {},
   "source": [
    "# Non-negative matrix factorization (NMF) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40186fa",
   "metadata": {},
   "source": [
    "NMF is a powerful sound source separation technique that can extract individual sound sources from a mixture of sounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed341d39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
